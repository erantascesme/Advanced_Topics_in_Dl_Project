{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "<center><br><font size=6>Final Project</font><br>\n",
    "<font size=5>Advanced Topics in Deep Learning</font><br>\n",
    "<b><font size=4>Part A - EDA</font></b>\n",
    "<br><br>\n",
    "Authors: Ido Rappaport & Eran Tascesme \n",
    "</font></center>\n",
    "\n",
    "**Submission Details:**\n",
    "<font size=2>\n",
    "<br>Ido Rappaport, ID: 322891623\n",
    "<br>Eran Tascesme , ID: 205708720 </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from collections import Counter\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    set_seed,\n",
    "    DataCollatorWithPadding,\n",
    "    MarianMTModel,\n",
    "    MarianTokenizer\n",
    ")\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Configuration or side-effect code\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "**1. Load Data & Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- 1. Load Data & Overview ---------------\n",
    "def load_and_inspect(path):\n",
    "    \"\"\"\n",
    "    Loads dataset and prints basic info.\n",
    "    \"\"\"\n",
    "    print(\"Loading dataset:\", path)\n",
    "    df = pd.read_csv(path, encoding='latin1')\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"Sample rows:\")\n",
    "    display(df.head())\n",
    "\n",
    "    print(\"\\n--- Missingness per column ---\")\n",
    "    missing = df.isnull().sum().sort_values(ascending=False)\n",
    "    display(missing)\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Configuration \n",
    "DATA_PATH = \"Corona_NLP_train.csv\" \n",
    "\n",
    "# 1. Load data\n",
    "df = load_and_inspect(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "The data set contains 6 columns, only the location column has missing values. We can conclude that the column most related to the course is 'OriginalTweet', but before we explore it, we will examine whether the other columns have an effect on 'Sentiment'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "**UserName and ScreenName Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for duplicates\n",
    "def check_uniqueness(column):\n",
    "    num_unique = column.nunique()\n",
    "    total_count = column.count()\n",
    "    if num_unique < total_count:\n",
    "        return \"has duplicates\"\n",
    "    else:\n",
    "        return \"all values are unique\"\n",
    "\n",
    "# Check the 'UserName' column\n",
    "user_name_result = check_uniqueness(df['UserName'])\n",
    "\n",
    "# Check the 'ScreenName' column\n",
    "screen_name_result = check_uniqueness(df['ScreenName'])\n",
    "\n",
    "print(f\"The 'UserName' column {user_name_result}.\")\n",
    "print(f\"The 'ScreenName' column {screen_name_result}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "There are no duplicates values, so it can be concluded that there is no influence between these columns and 'Sentiment', meaning they can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "**TweetAt Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- Temporal Patterns ---------------\n",
    "def temporal_trends(df):\n",
    "    # Convert TweetAt column to datetime (try multiple formats)\n",
    "    if 'TweetAt' not in df.columns:\n",
    "        print(\"No TweetAt column found.\")\n",
    "        return\n",
    "    # try parsing, tolerate failures\n",
    "    def parse_date(x):\n",
    "        for fmt in (\"%d-%m-%Y %H:%M:%S\", \"%d-%m-%Y %H:%M\", \"%Y-%m-%d %H:%M:%S\", \"%d-%b-%Y\"):\n",
    "            try:\n",
    "                return datetime.strptime(x, fmt)\n",
    "            except Exception:\n",
    "                continue\n",
    "        # fallback\n",
    "        try:\n",
    "            return pd.to_datetime(x)\n",
    "        except Exception:\n",
    "            return pd.NaT\n",
    "    df['TweetAt_dt'] = df['TweetAt'].astype(str).apply(parse_date)\n",
    "    # Drop rows without dates\n",
    "    df_time = df.dropna(subset=['TweetAt_dt']).copy()\n",
    "    df_time['date'] = df_time['TweetAt_dt'].dt.date\n",
    "    # daily counts\n",
    "    daily = df_time.groupby('date')['OriginalTweet'].count()\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(daily.index, daily.values)\n",
    "    plt.title(\"Tweets per day\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    # Sentiment over time (stacked) - compute counts per date+sentiment\n",
    "    pivot = df_time.pivot_table(index='date', columns='Sentiment', values='OriginalTweet', aggfunc='count', fill_value=0)\n",
    "    # Plot stacked area (each sentiment a line) - using matplotlib only\n",
    "    plt.figure(figsize=(12,5))\n",
    "    for col in pivot.columns:\n",
    "        plt.plot(pivot.index, pivot[col], label=col)\n",
    "    plt.legend()\n",
    "    plt.title(\"Sentiment counts over time\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    return df_time\n",
    "\n",
    "\n",
    "# 4. Temporal trends\n",
    "df_time = temporal_trends(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Based on the Plots, it's clear that there is no correlation between a specific time and a specific sentiment. Although there are periods with more or fewer tweets, the distribution of sentiments appears very similar across all time periods. Therefore, we can assume that 'TweetAt' has no influence and can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "**Location Exploration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Initially, we attempted to examine the relationship between Location and Sentiment by creating a graph that displayed the quantity of each sentiment versus a specific location. We found that the code was not working and showed numerous errors. We realized we needed to dive deeper and discovered that there was a lot of \"irrelevant\" text for the location or locations that contained various symbols. This led us to the conclusion that the data was dirty.\n",
    "\n",
    "To address this, we checked how many data points there were for each word count and tried to see if the word count was related to a particular sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 'Location_len_words' column that counts the words of the Location.\n",
    "df['Location_len_words'] = df['Location'].astype(str).apply(lambda x: len(str(x).split()))\n",
    "\n",
    "display(df['Location_len_words'].describe())\n",
    "\n",
    "# Count the occurrences of each number\n",
    "counts = df['Location_len_words'].value_counts().sort_index()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "counts.plot(kind='bar')\n",
    "plt.title('Amount of Rows for Each Location Length')\n",
    "plt.xlabel('Location Length (Words)')\n",
    "plt.ylabel('Number of Rows')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Filter the DataFrame for rows where 'Location_len_words' is bigger than 5\n",
    "filtered_df = df[df['Location_len_words'] > 5]\n",
    "\n",
    "# Get the count of rows for each sentiment in the filtered data\n",
    "sentiment_counts = filtered_df['Sentiment'].value_counts().sort_index()\n",
    "\n",
    "# Plot the results as a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sentiment_counts.plot(kind='bar')\n",
    "plt.title('Number of Rows for Each Sentiment with Location Length > 5')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Rows')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "We found that the distribution of long location names was similar to the overall data distribution, which suggests that Location length has no impact on Sentiment. However, we'll consider adding location as an additional feature in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "**Sentiment Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nValue counts for 'Sentiment':\")\n",
    "display(df['Sentiment'].value_counts())\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "counts = df['Sentiment'].value_counts().sort_values(ascending=False)\n",
    "counts.plot(kind='bar')\n",
    "plt.title(\"Sentiment distribution\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "The dataset is unbalanced, with a significant disparity of nearly 50% between positive and extremely negative sentiments.\n",
    "\n",
    "To address this, we will likely use text augmentation to increase the size of the minority classes. The specific augmentation techniques will be chosen based on the final model we select. Additionally, we might use a class-weighted loss function to penalize the model more heavily for misclassifying minority classes, thereby helping to mitigate the imbalance issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "**OriginalTweet Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 5 rows\n",
    "random_rows = df.sample(n=5)\n",
    "\n",
    "# Print the 'OriginalTweet' column from the random rows\n",
    "print(\"Randomly selected 'OriginalTweet' values:\")\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "for tweet in random_rows['OriginalTweet']:\n",
    "    print(f\"- {tweet}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "We can observe a significant variety of different texts, both in terms of length and the content they contain. The dataset contains various elements like URLs, mentions, emojis, and hashtags.\n",
    "1. We will begin by investigating the tweets length patterns\n",
    "2. We will investigating the impact of each of the elements above on the data and make decisions accordingly.\n",
    "3. We will explore certain Stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "**Tweet length patterns**\n",
    "\n",
    "We found that the number of words in the tweets ranges from 1 to 64, which is a wide range. The length of the tweets also varies significantly, from 11 to 355 characters.\n",
    "\n",
    "Approximately 10-20% of the tweets can be defined as outliers at the ends of the distribution. However, the sentiment distribution appears stable regarding word count. The neutral sentiment is an exception, behaving opposite to the others, but its tweet count isn't an outlier compared to the rest.\n",
    "\n",
    "We will take the following into consideration for future steps:\n",
    "\n",
    "The ability to input tweets into a classification model\n",
    "\n",
    "The choice of padding and tokenizer\n",
    "\n",
    "We'll likely split the data into two sets. One will have the outliers removed to reduce noise, while the other will retain them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet length stats (raw original tweets)\n",
    "df['orig_len_chars'] = df['OriginalTweet'].astype(str).apply(len)\n",
    "df['orig_len_words'] = df['OriginalTweet'].astype(str).apply(lambda x: len(str(x).split()))\n",
    "print(\"\\nTweet length (chars) - describe:\")\n",
    "display(df['orig_len_chars'].describe())\n",
    "print(\"\\nTweet length (words) - describe:\")\n",
    "display(df['orig_len_words'].describe())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df, x='orig_len_words', hue='Sentiment', bins=50, kde=True, legend=True)\n",
    "\n",
    "plt.title('Words Amount Distribution')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df, x='orig_len_chars', hue='Sentiment', bins=50, kde=True, legend=True)\n",
    "\n",
    "plt.title('Chars Length Distribution')\n",
    "plt.xlabel('Number of chars')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "**URLs Exploration**\n",
    "\n",
    "URLs (e.g., https://t.co) often don't carry sentiment themselves but can indicate shared resources.\n",
    "We check how many tweets contain URLs and if their presence is correlated with a specific sentiment.\n",
    "\n",
    "We find that the distribution appears similar to the regular distribution,\n",
    "so we can conclude that URLs are not correlated with a specific sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_urls(df, text_col='OriginalTweet', sentiment_col='Sentiment', top_n=10):\n",
    "    # Regex to capture URLs\n",
    "    url_pattern = r'https?://\\S+'\n",
    "    \n",
    "    # Extract URLs from tweets\n",
    "    df['urls'] = df[text_col].apply(lambda x: re.findall(url_pattern, x))\n",
    "    \n",
    "    # Count tweets containing URLs\n",
    "    tweets_with_urls = df['urls'].apply(len) > 0\n",
    "    print(f\"Tweets containing URLs: {tweets_with_urls.sum()} / {len(df)} \"\n",
    "          f\"({tweets_with_urls.mean():.2%})\")\n",
    "    \n",
    "    # Extract domains\n",
    "    def extract_domain(url):\n",
    "        try:\n",
    "            return urlparse(url).netloc.replace(\"www.\", \"\")\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    df['domains'] = df['urls'].apply(lambda urls: [extract_domain(u) for u in urls if extract_domain(u)])\n",
    "    \n",
    "    # Flatten all domains to find top ones\n",
    "    all_domains = [d for sublist in df['domains'] for d in sublist]\n",
    "    domain_counts = pd.Series(all_domains).value_counts().head(top_n)\n",
    "    print(\"\\nTop domains:\\n\", domain_counts)\n",
    "    \n",
    "    # Filter dataframe to only include top domains\n",
    "    df_exploded = df.explode('domains')\n",
    "    df_exploded = df_exploded[df_exploded['domains'].isin(domain_counts.index)]\n",
    "    \n",
    "    # Plot: top domains per sentiment\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(\n",
    "        data=df_exploded,\n",
    "        x='domains',\n",
    "        hue=sentiment_col,\n",
    "        order=domain_counts.index\n",
    "    )\n",
    "    plt.title(f\"Top {top_n} most frequent domains per sentiment\")\n",
    "    plt.xlabel(\"Domain\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"Sentiment\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "explore_urls(df, text_col='OriginalTweet', sentiment_col='Sentiment', top_n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "**Mentions Exploration**\n",
    "\n",
    "Mentions (e.g., @username) are common in tweets and can be a sign of a reply or a reference to a specific user.\n",
    "We check the number of tweets with Mentions and if a specific user tend to be more positive or negative.\n",
    "\n",
    "Generally, the distribution appears similar to the regular one, suggesting there's no correlation between a specific mention and a sentiment. However, there are a few exceptions. For example, YouTube has a stronger link to a neutral sentiment, while CNN is more strongly connected to a negative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mentions\n",
    "df['mentions'] = df['OriginalTweet'].apply(lambda x: re.findall(r'@(\\w+)', x))\n",
    "\n",
    "# Count tweets containing at least one mention\n",
    "num_with_mentions = (df['mentions'].apply(len) > 0).sum()\n",
    "print(f\"Number of tweets with mentions: {num_with_mentions}\")\n",
    "print(f\"Percentage of tweets with mentions: {num_with_mentions / len(df) * 100:.2f}%\")\n",
    "\n",
    "# Overall most common mentions\n",
    "all_mentions = [mention for mentions_list in df['mentions'] for mention in mentions_list]\n",
    "mention_counts = Counter(all_mentions)\n",
    "print(\"\\nTop 10 most mentioned usernames overall:\")\n",
    "print(mention_counts.most_common(10))\n",
    "\n",
    "\n",
    "# Explode mentions into separate rows\n",
    "mentions_df = df.explode('mentions')\n",
    "\n",
    "# Drop rows without mentions\n",
    "mentions_df = mentions_df.dropna(subset=['mentions'])\n",
    "\n",
    "# Count total mentions\n",
    "mention_counts = mentions_df['mentions'].value_counts()\n",
    "\n",
    "# Take top 15 mentioned usernames\n",
    "top_mentions = mention_counts.head(15).index\n",
    "\n",
    "# Filter to top mentions only\n",
    "top_mentions_df = mentions_df[mentions_df['mentions'].isin(top_mentions)]\n",
    "\n",
    "# Group by mention and sentiment\n",
    "mention_sentiment_counts = (\n",
    "    top_mentions_df\n",
    "    .groupby(['mentions', 'Sentiment'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "\n",
    "# Pivot for plotting\n",
    "mention_sentiment_pivot = mention_sentiment_counts.pivot(index='mentions', columns='Sentiment', values='count').fillna(0)\n",
    "\n",
    "# Sort by total mentions\n",
    "mention_sentiment_pivot = mention_sentiment_pivot.loc[top_mentions]\n",
    "\n",
    "# Plot\n",
    "mention_sentiment_pivot.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(12, 7),\n",
    "    colormap='tab20'\n",
    ")\n",
    "\n",
    "plt.title('Top 15 Most Mentioned Usernames by Sentiment', fontsize=16)\n",
    "plt.xlabel('Username', fontsize=12)\n",
    "plt.ylabel('Number of Mentions', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Sentiment')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "**Emojis Exploration**\n",
    "\n",
    "Emojis are powerful conveyors of sentiment in social media. \n",
    "\n",
    "We find that there are just 2 emojies and the total number of emojies is 70. so it doesnt influence on sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "    \"\"\"\n",
    "    Extracts all emojis from a given string.\n",
    "    Returns a list of emojis found.\n",
    "    \"\"\"\n",
    "    return [c for c in text if c in emoji.EMOJI_DATA]\n",
    "\n",
    "# Create a list of all emojis found in the 'OriginalTweet' column\n",
    "all_emojis = df['OriginalTweet'].apply(extract_emojis).explode().dropna().tolist()\n",
    "\n",
    "# Calculate the total number of emojis\n",
    "total_emojis = len(all_emojis)\n",
    "\n",
    "# Get the count of each unique emoji\n",
    "emoji_counts = Counter(all_emojis)\n",
    "\n",
    "print(f\"Total number of emojis in the dataset: {total_emojis}\\n\")\n",
    "print(\"Top 10 most frequent emojis:\")\n",
    "\n",
    "for emoji, count in emoji_counts.most_common(10):\n",
    "    print(f\"- {emoji} : {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "**Hashtags Exploration**\n",
    "\n",
    "Hashtags (e.g., #COVID19) categorize topics and are crucial for understanding the context of a tweet.\n",
    "We find that the distribution seems to follow the regular one, with no hashtags being overly correlated with a specific sentiment. However, some hashtags like #toiletpaper and #coronacrisis show a slightly different distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract hashtags from a tweet\n",
    "def extract_hashtags(text):\n",
    "    return re.findall(r\"#\\w+\", text)\n",
    "\n",
    "# Apply extraction\n",
    "df['hashtags'] = df['OriginalTweet'].apply(extract_hashtags)\n",
    "\n",
    "# Count tweets that contain hashtags\n",
    "tweets_with_hashtags = df[df['hashtags'].str.len() > 0]\n",
    "print(f\"Tweets with hashtags: {len(tweets_with_hashtags)} / {len(df)}\")\n",
    "\n",
    "# Flatten hashtags into a list\n",
    "all_hashtags = [ht.lower() for sublist in df['hashtags'] for ht in sublist]\n",
    "\n",
    "# Top hashtags\n",
    "top_hashtags = Counter(all_hashtags).most_common(15)\n",
    "top_hashtags_list = [ht for ht, _ in top_hashtags]\n",
    "\n",
    "# Filter only rows with top hashtags\n",
    "df_top_hashtags = df[df['hashtags'].apply(lambda x: any(ht.lower() in top_hashtags_list for ht in x))]\n",
    "\n",
    "# Expand hashtags into multiple rows (one hashtag per row for sentiment grouping)\n",
    "rows = []\n",
    "for _, row in df_top_hashtags.iterrows():\n",
    "    for ht in row['hashtags']:\n",
    "        if ht.lower() in top_hashtags_list:\n",
    "            rows.append({'hashtag': ht.lower(), 'Sentiment': row['Sentiment']})\n",
    "df_hashtags_expanded = pd.DataFrame(rows)\n",
    "\n",
    "# Plot sentiment counts for top hashtags\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.countplot(\n",
    "    data=df_hashtags_expanded,\n",
    "    y='hashtag',\n",
    "    hue='Sentiment',\n",
    "    order=[ht for ht, _ in top_hashtags],\n",
    "    palette='Set2'\n",
    ")\n",
    "plt.title(\"Top 15 Hashtags by Sentiment\")\n",
    "plt.xlabel(\"Tweet Count\")\n",
    "plt.ylabel(\"Hashtag\")\n",
    "plt.legend(title=\"Sentiment\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "**Stop Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "First, we will check which words appear most frequently and if they have any connection to sentiment.\n",
    "We will analyze the words after removing URLs, mentions, and punctuation.\n",
    "\n",
    "As expected, most of the most frequent words were stop words like \"the,\" \"is,\" \"on,\" and \"if.\"\n",
    "\n",
    "After removing these words, we found the most common terms are \"corona\", \"market\", \"price\", \"grocery\", and \"food\". These words appear most frequently across all sentiments, so we cannot assume a specific correlation between these leading words and any particular sentiment. We will decide later whether to keep or remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    # Lowercase, remove urls, mentions, hashtags (or keep hashtag text), punctuation\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    # Replace URLs and mentions\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'@\\w+', ' ', text)\n",
    "    # Remove punctuation except keep apostrophes\n",
    "    text = re.sub(r\"[^a-z0-9'\\s]\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --------------- 5. Text Cleaning & Token-Level Analysis ---------------\n",
    "def token_analysis(df, text_col='OriginalTweet', top_n=30):\n",
    "    df['clean'] = df[text_col].astype(str).map(text_clean)\n",
    "    # Tokenize and compute token stats\n",
    "    df['tokens'] = df['clean'].map(lambda x: word_tokenize(x))\n",
    "    df['num_tokens'] = df['tokens'].map(len)\n",
    "    # vocabulary & top unigrams\n",
    "    all_tokens = [t for toks in df['tokens'] for t in toks]\n",
    "    freq = collections.Counter(all_tokens)\n",
    "    top = freq.most_common(top_n)\n",
    "    top_df = pd.DataFrame(top, columns=['token','count'])\n",
    "    print(f\"\\nTop {10} tokens (including stopwords):\")\n",
    "    display(top_df.head(10))\n",
    "    # Plot top tokens\n",
    "    plt.figure(figsize=(10,5))\n",
    "    top_words = top_df.set_index('token').head(25)\n",
    "    top_words.plot(kind='bar', legend=False, ax=plt.gca())\n",
    "    plt.title(\"Top tokens (including stopwords)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "    return df, freq\n",
    "\n",
    "\n",
    "# Text cleaning & token analysis\n",
    "df, freq = token_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- Stopword Removal & Filtered Analysis ---------------\n",
    "def filtered_token_analysis(df, freq, top_n=30):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    # Optionally extend stopwords with pandemic filler words if desired\n",
    "    filtered_tokens = [w for w in freq.keys() if w not in stop and len(w)>2]\n",
    "    filtered_freq = {w:freq[w] for w in filtered_tokens}\n",
    "    top_filtered = sorted(filtered_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    topf_df = pd.DataFrame(top_filtered, columns=['token','count'])\n",
    "    print(f\"\\nTop {10} tokens after stopword removal:\")\n",
    "    display(topf_df.head(10))\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10,5))\n",
    "    topf_df.set_index('token').plot(kind='bar', legend=False, ax=plt.gca())\n",
    "    plt.title(\"Top tokens (stopwords removed)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "    return topf_df\n",
    "\n",
    "\n",
    "# 6. Filtered analysis\n",
    "topf_df = filtered_token_analysis(df, freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- Word Clouds per Sentiment ---------------\n",
    "def wordclouds_by_sentiment(df, text_col='clean'):\n",
    "    sentiments = df['Sentiment'].unique()\n",
    "    for s in sentiments:\n",
    "        text = \" \".join(df.loc[df['Sentiment']==s, text_col].astype(str).values)\n",
    "        wc = WordCloud(width=800, height=400, max_words=150).generate(text)\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"WordCloud for sentiment: {s}\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# 7. Wordclouds per sentiment\n",
    "wordclouds_by_sentiment(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "We concluded that there is likely no strong correlation between a specific sentiment and a particular feature. We also observed that elements such as URLs, emojis, and mentions are not concentrated within a specific sentiment. Additionally, stop words appeared most frequently and are not tied to any particular sentiment. We therefore believe these elements introduce noise into the data and can be removed.\n",
    "\n",
    "We also found that while the data is reasonably distributed, it is imbalanced. This could lead to a bias towards a certain sentiment and harm generalizability.\n",
    "\n",
    "Given these findings, it would be interesting to evaluate the results of the classifiers after training on data that has been cleaned of the aforementioned elements. We should also consider data augmentation for the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "**Examine the word count distribution after removing elements and stop words.**\n",
    "\n",
    "We can see that there are tails with few samples, which could introduce noise.\n",
    "\n",
    "There also appears to be a tendency for the \"neutral\" sentiment to be associated with a smaller number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "df['tokens_no_stop'] = df['tokens'].map(lambda x: [word for word in x if word not in stop])\n",
    "df['clean_no_stop'] = df['tokens_no_stop'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet length stats (raw original tweets)\n",
    "df['clean_len_words'] = df['clean_no_stop'].astype(str).apply(lambda x: len(str(x).split()))\n",
    "print(\"\\nTweet length (words) - describe:\")\n",
    "display(df['clean_len_words'].describe())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df, x='clean_len_words', hue='Sentiment', bins=50, kde=True, legend=True)\n",
    "\n",
    "plt.title('Words Amount Distribution')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['clean_len_words'] < 4][['clean_no_stop', 'Sentiment']].head(20))    # showing the tendency to Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet length stats (raw original tweets)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df[df['clean_len_words'] < 4], x='clean_len_words', hue='Sentiment', bins=4, kde=False, legend=True)\n",
    "\n",
    "plt.title('Words Amount Distribution (words less then 4)')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "df_train_clean = df.rename(columns={'clean_no_stop': 'text', 'Sentiment': 'label'})\n",
    "df_train_clean = df_train_clean[['text', 'label']]\n",
    "df_train_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "**Cut tails**\n",
    "\n",
    "Due to the fact that the tails may introduce noise, yet still show a preference for a specific sentiment, we will consider training the models on both the full dataset and a trimmed one without the tails.\n",
    "\n",
    "It appears that after removing the tails, the data is distributed more smoothly, without bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cutted = df_train_clean.copy()\n",
    "df_train_cutted[\"word_count\"] = df_train_cutted[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "df_train_cutted = df_train_cutted[(df_train_cutted[\"word_count\"] >= 6) &\n",
    "                          (df_train_cutted[\"word_count\"] <= 30)]\n",
    "\n",
    "print(f\"Original size: {len(df_train_clean)}, After filtering: {len(df_train_cutted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet Words length stats (raw original tweets)\n",
    "print(\"\\nTweet number of words - describe:\")\n",
    "display(df_train_cutted['word_count'].describe())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df_train_cutted, x='word_count', hue='label', bins=50, kde=True, legend=True)\n",
    "\n",
    "plt.title('Words Amount Distribution')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Tweet Chars length stats (raw original tweets)\n",
    "df_train_cutted['orig_len_chars'] = df_train_cutted['text'].astype(str).apply(len)\n",
    "\n",
    "print(\"\\nTweet length - describe:\")\n",
    "display(df_train_cutted['orig_len_chars'].describe())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df_train_cutted, x='orig_len_chars', hue='label', bins=50, kde=True, legend=True)\n",
    "\n",
    "plt.title('Chars Length Distribution')\n",
    "plt.xlabel('Number of chars')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the helper column and reset index\n",
    "df_train_cutted = df_train_cutted[['text', 'label']]\n",
    "df_train_cutted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "<b> Processing and saving </b>\n",
    "\n",
    "We've decided to save four types of files and split them to train/val:\n",
    "\n",
    "1. Normal - train_dirty.csv\n",
    "\n",
    "2. Cleaned of elements and stop words - train_clean.csv\n",
    "\n",
    "3. Trimmed of tails - train_cutted.csv\n",
    "\n",
    "4. With augmentation - train_balanced.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_train_val(df, train_name, val_name):\n",
    "    sentiment_map = {\n",
    "        \"Negative\": 0,\n",
    "        \"Neutral\": 1,\n",
    "        \"Positive\": 2,\n",
    "        \"Extremely Positive\": 3,\n",
    "        \"Extremely Negative\": 4\n",
    "    }\n",
    "\n",
    "    df[\"label\"] = df[\"label\"].map(sentiment_map)\n",
    "\n",
    "    train, val = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "    train.to_csv(train_name, index=False)\n",
    "    val.to_csv(val_name, index=False)\n",
    "\n",
    "    print(f\"save {train_name} and {val_name}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_dirty = df[['OriginalTweet','Sentiment']]\n",
    "df_train_dirty = df_train_dirty.rename(columns={'OriginalTweet': 'text', 'Sentiment': 'label'})\n",
    "df_train_dirty.head()\n",
    "\n",
    "save_train_val(df_train_dirty, \"train_dirty.csv\", \"val_dirty.csv\")\n",
    "save_train_val(df_train_clean, \"train_clean.csv\", \"val_clean.csv\")\n",
    "save_train_val(df_train_cutted, \"train_cutted.csv\", \"val_cutted.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "<b>Augmentation</b> (just on train set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- SETTINGS ----\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "df = df_train_cutted.copy()  # DataFrame with 'text','label'\n",
    "\n",
    "# ---- 1) Contextual Word Embedding Augmenter ----\n",
    "ctx_aug = naw.ContextualWordEmbsAug(\n",
    "    model_path=\"xlm-roberta-base\",\n",
    "    action=\"substitute\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# ---- 2) Manual Back-Translation (Hebrew <-> English) ----\n",
    "def translate(texts, model_name):\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    translated = model.generate(**inputs)\n",
    "    return [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "\n",
    "def back_translate_he_en_he(text):\n",
    "    # Hebrew -> English\n",
    "    en_text = translate([text], \"Helsinki-NLP/opus-mt-he-en\")[0]\n",
    "    # English -> Hebrew\n",
    "    he_text = translate([en_text], \"Helsinki-NLP/opus-mt-en-he\")[0]\n",
    "    return he_text\n",
    "\n",
    "# ---- 3) Combine augmenters ----\n",
    "def augment_text_once(text):\n",
    "    out = text\n",
    "    try:\n",
    "        out = back_translate_he_en_he(out)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        out = ctx_aug.augment(out)\n",
    "        if isinstance(out, list):\n",
    "            out = out[0]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return out\n",
    "\n",
    "# ---- Balance dataset ----\n",
    "counts = Counter(df[\"label\"].tolist())\n",
    "majority_label, majority_count = max(counts.items(), key=lambda kv: kv[1])\n",
    "\n",
    "rows = []\n",
    "for y, n in counts.items():\n",
    "    if n >= majority_count:\n",
    "        continue\n",
    "    need = majority_count - n\n",
    "    subset = df[df.label == y][\"text\"].tolist()\n",
    "    base_pool = subset if len(subset) > 0 else [\"טקסט חסר\"]\n",
    "    for _ in range(need):\n",
    "        src = random.choice(base_pool)\n",
    "        gen = augment_text_once(src)\n",
    "        rows.append({\"text\": gen, \"label\": y})\n",
    "\n",
    "df_balanced = pd.concat([df, pd.DataFrame(rows)], ignore_index=True)\n",
    "df_balanced.to_csv(\"train_balanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nValue counts for 'Sentiment':\")\n",
    "display(df_balanced['label'].value_counts())\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "counts = df_balanced['label'].value_counts().sort_values(ascending=False)\n",
    "counts.plot(kind='bar')\n",
    "plt.title(\"Sentiment distribution\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "<b>Feeling the test data</b>\n",
    "\n",
    "the distributio of the sentiments in the test is equaly to train.\n",
    "\n",
    "In addition, we saved a test_clean for models that training on train_clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"Corona_NLP_test.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "print(f\"\\nValue counts for 'Sentiment':\")\n",
    "display(test_data['Sentiment'].value_counts())\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "counts = test_data['Sentiment'].value_counts().sort_values(ascending=False)\n",
    "counts.plot(kind='bar')\n",
    "plt.title(\"Sentiment distribution\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    # Lowercase, remove urls, mentions, hashtags (or keep hashtag text), punctuation\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    # Replace URLs and mentions\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'@\\w+', ' ', text)\n",
    "    # Remove punctuation except keep apostrophes\n",
    "    text = re.sub(r\"[^a-z0-9'\\s]\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "test_data['clean'] = test_data['OriginalTweet'].astype(str).map(text_clean)\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "test_data['tokens'] = test_data['clean'].map(lambda x: word_tokenize(x))\n",
    "test_data['tokens_no_stop'] = test_data['tokens'].map(lambda x: [word for word in x if word not in stop])\n",
    "test_data['clean_no_stop'] = test_data['tokens_no_stop'].map(lambda x: ' '.join(x))\n",
    "\n",
    "test_data = test_data.rename(columns={'clean_no_stop': 'text', 'Sentiment': 'label'})\n",
    "test_data = test_data[['text', 'label']]\n",
    "\n",
    "sentiment_map = {\n",
    "    \"Negative\": 0,\n",
    "    \"Neutral\": 1,\n",
    "    \"Positive\": 2,\n",
    "    \"Extremely Positive\": 3,\n",
    "    \"Extremely Negative\": 4\n",
    "}\n",
    "\n",
    "test_data[\"label\"] = test_data[\"label\"].map(sentiment_map)\n",
    "test_data[\"label\"] = test_data[\"label\"].astype(int)\n",
    "test_data = test_data[['text', 'label']]\n",
    "\n",
    "test_data.to_csv(\"test_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "<h2>END</h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ProENV)",
   "language": "python",
   "name": "proenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
