{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNZUnV7E7Zj6YVa1P/hSaz+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","<center><br><font size=6>Final Project</font><br>\n","<font size=5>Advanced Topics in Deep Learning</font><br>\n","<b><font size=4>Part B</font></b>\n","<br><font size=4>Training Models like Excercise 5</font><br><br>\n","Authors: Ido Rappaport & Eran Tascesme\n","</font></center>\n","\n","**Submission Details:**\n","<font size=2>\n","<br>Ido Rappaport, ID: 322891623\n","<br>Eran Tascesme , ID: 205708720 </font>\n"],"metadata":{"id":"NvpYI-0WgaUl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HnMlj2ID4OGH"},"outputs":[],"source":["''''\n","!pip install optuna\n","!pip install wandb\n","!pip install nlpaug\n","!pip install gensim\n","!pip install evaluate\n","'''"]},{"cell_type":"markdown","source":["**Import libraries**\n","\n","❗Note the versions of the packages, we have included information in requirements.txt❗"],"metadata":{"id":"ZGXi6ds5gf73"}},{"cell_type":"code","source":["# Standard libraries\n","import os\n","import re\n","import string\n","import random\n","import warnings\n","from collections import Counter\n","\n","# Data handling and visualization\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from pathlib import Path\n","\n","# NLP libraries\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","import nlpaug.augmenter.word as naw\n","import nlpaug.augmenter.sentence as nas\n","from gensim import corpora, models\n","from urllib.parse import urlparse\n","\n","# Machine learning and deep learning\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from torch import nn, optim\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import (\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    accuracy_score,\n","    classification_report,\n","    confusion_matrix,\n","    ConfusionMatrixDisplay\n",")\n","\n","# Hugging Face Transformers\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n","    EarlyStoppingCallback,\n","    set_seed,\n","    TrainerCallback,\n","    TrainerState,\n","    TrainerControl,\n","    DataCollatorWithPadding,\n","    RobertaForSequenceClassification,\n","    MarianMTModel,\n","    MarianTokenizer\n",")\n","from datasets import Dataset, DatasetDict, load_dataset\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","import evaluate\n","\n","# Other libraries\n","import optuna\n","import wandb\n","from tqdm import tqdm\n","\n","# Filter warnings\n","warnings.filterwarnings('ignore')\n","\n","# Download NLTK resources\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    nltk.download('punkt')\n","try:\n","    nltk.data.find('corpora/stopwords')\n","except LookupError:\n","    nltk.download('stopwords')"],"metadata":{"id":"ITeJ8qym4WFi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"id":"Osy-xhh94Xla"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ziDUir484Z4F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import login\n","login('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')"],"metadata":{"id":"03_clmXj4a6r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Load CSV Files**\n","\n","Following the results from training based on excercise 4, we concluded that we can train solely on the clean, truncated dataset after augmentation. This approach also helps save time and resources."],"metadata":{"id":"IRHphkGGg5tB"}},{"cell_type":"code","source":["# Load CSV files\n","\n","drive_path = \"/content/drive/My Drive/Colab Notebooks/\"\n","\n","train_dataset = pd.read_csv(drive_path + \"train_balanced.csv\", encoding=\"ISO-8859-1\")\n","eval_dataset = pd.read_csv(drive_path + \"val_balanced.csv\", encoding=\"ISO-8859-1\")"],"metadata":{"id":"9cahbmKSgrY6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Training Classes and Methods**\n","\n","The function `train_with_optuna_wandb` is designed for training a Hugging Face `transformers` model using hyperparameter optimization with Optuna and experiment tracking with Weights & Biases (W&B). It performs the following steps:\n","\n","*   Sets up W&B for tracking the Optuna trials and the final best model run.\n","*   Initializes the tokenizer and prepares the datasets.\n","*   Defines the model initialization, metric computation, and objective function for Optuna.\n","*   Configures base training arguments for the hyperparameter search.\n","*   Implements a custom callback to log metrics per epoch during Optuna trials to W&B.\n","*   Defines the hyperparameter search space for Optuna.\n","*   Runs the Optuna hyperparameter search to find the best combination of hyperparameters.\n","*   Prints the details of the best trial found by Optuna.\n","*   Logs a summary table of all Optuna trials to W&B.\n","*   Performs a final training run with the best hyperparameters found by Optuna, with W&B logging enabled.\n","*   Saves the trained model with the best hyperparameters.\n","\n","This function provides a **general framework** for hyperparameter tuning and experiment tracking for sequence classification tasks using Hugging Face models, Optuna, and W&B."],"metadata":{"id":"k9aQ2l3xhq8K"}},{"cell_type":"code","source":["def train_with_optuna_wandb(\n","    project_name, model_name, train_dataset, eval_dataset,\n","    num_labels=5, n_trials=5, num_train_epochs=5\n","):\n","    # Set seed for reproducibility\n","    set_seed(42)\n","\n","    # Set W&B environment\n","    os.environ[\"WANDB_PROJECT\"] = project_name\n","    os.environ[\"WANDB_MODE\"] = \"disabled\"  # Disable W&B auto-logging for trials\n","\n","    # Start single W&B run to track all trials\n","    wandb_run = wandb.init(project=project_name, name=\"optuna_search_all_trials\", reinit=True)\n","\n","    # Define custom metrics for step tracking\n","    wandb.define_metric(\"epoch\")\n","    wandb.define_metric(\"eval_accuracy\", step_metric=\"epoch\")\n","    wandb.define_metric(\"train_accuracy\", step_metric=\"epoch\")\n","\n","    # W&B table for final summary\n","    trials_table = wandb.Table(columns=[\n","        \"trial\", \"learning_rate\", \"batch_size\", \"weight_decay\", \"eval_accuracy\", \"train_accuracy\"\n","    ])\n","\n","    # Tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","    def tokenize_function(example):\n","        return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n","\n","    tokenized_train = train_dataset.map(tokenize_function, batched=True, batch_size=64)\n","    tokenized_eval = eval_dataset.map(tokenize_function, batched=True, batch_size=64)\n","\n","    # Accuracy metric\n","    metric = evaluate.load(\"accuracy\")\n","\n","    def model_init():\n","        return AutoModelForSequenceClassification.from_pretrained(\n","            model_name, num_labels=num_labels, ignore_mismatched_sizes=True\n","        )\n","\n","    def compute_metrics(eval_pred):\n","        predictions = eval_pred.predictions.argmax(axis=-1)\n","        labels = eval_pred.label_ids\n","        return metric.compute(predictions=predictions, references=labels)\n","\n","    def compute_objective(metrics):\n","        return metrics[\"eval_accuracy\"]\n","\n","    # Base training args (for Optuna search)\n","    base_training_args = TrainingArguments(\n","        output_dir=f\"/content/drive/MyDrive/Colab Notebooks/{project_name}/temp_run\",\n","        eval_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","        logging_strategy=\"epoch\",\n","        num_train_epochs=num_train_epochs,\n","        report_to=[],  # Disable W&B logging during search\n","        logging_dir=f\"/content/drive/MyDrive/Colab Notebooks/{project_name}/logs\",\n","    )\n","\n","    # Callback for logging per epoch\n","    class WandbOptunaCallback(TrainerCallback):\n","        def on_epoch_end(self, args, state, control, **kwargs):\n","            train_metrics = trainer.evaluate(eval_dataset=tokenized_train, metric_key_prefix=\"train\")\n","            eval_metrics = trainer.evaluate(eval_dataset=tokenized_eval, metric_key_prefix=\"eval\")\n","\n","            train_acc = train_metrics.get(\"train_accuracy\", None)\n","            eval_acc = eval_metrics.get(\"eval_accuracy\", None)\n","\n","            # Log per epoch with trial info\n","            wandb.log({\n","                \"eval_accuracy\": eval_acc,\n","                \"train_accuracy\": train_acc,\n","                \"epoch\": state.epoch,\n","                \"trial\": state.trial_name,\n","            })\n","\n","            # Add final metrics to summary table\n","            if state.epoch + 1 == num_train_epochs:\n","                trials_table.add_data(\n","                    state.trial_name,\n","                    state.trial_params.get(\"learning_rate\"),\n","                    state.trial_params.get(\"per_device_train_batch_size\"),\n","                    state.trial_params.get(\"weight_decay\"),\n","                    eval_acc,\n","                    train_acc\n","                )\n","\n","    # Trainer for Optuna trials\n","    trainer = Trainer(\n","        model_init=model_init,\n","        args=base_training_args,\n","        train_dataset=tokenized_train,\n","        eval_dataset=tokenized_eval,\n","        tokenizer=tokenizer,\n","        compute_metrics=compute_metrics,\n","        callbacks=[WandbOptunaCallback()]\n","    )\n","\n","    def optuna_hp_space(trial):\n","        return {\n","            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n","            \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [64, 128]),\n","            \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-4, 0.3),\n","        }\n","\n","    # Run hyperparameter search\n","    best_run = trainer.hyperparameter_search(\n","        direction=\"maximize\",\n","        backend=\"optuna\",\n","        hp_space=optuna_hp_space,\n","        n_trials=n_trials,\n","        compute_objective=compute_objective,\n","        study_name=\"transformers_optuna_study\",\n","        storage=f\"sqlite:////content/drive/MyDrive/Colab Notebooks/{project_name}/optuna_trials.db\",\n","        load_if_exists=True\n","    )\n","\n","    print(\"Best trial:\", best_run)\n","\n","    # Log summary table\n","    wandb.log({\"optuna_trials\": trials_table})\n","\n","    # Finish main W&B run\n","    wandb.finish()\n","\n","    # Re-enable W&B for final training run\n","    os.environ[\"WANDB_MODE\"] = \"online\"\n","\n","    # Final training args (W&B enabled)\n","    final_training_args = TrainingArguments(\n","        output_dir=f\"/content/drive/MyDrive/Colab Notebooks/{project_name}/best_model_run\",\n","        eval_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","        logging_strategy=\"epoch\",\n","        num_train_epochs=num_train_epochs,\n","        learning_rate=best_run.hyperparameters[\"learning_rate\"],\n","        per_device_train_batch_size=best_run.hyperparameters[\"per_device_train_batch_size\"],\n","        weight_decay=best_run.hyperparameters[\"weight_decay\"],\n","        report_to=[\"wandb\"],\n","        logging_dir=f\"/content/drive/MyDrive/Colab Notebooks/{project_name}/logs\",\n","        run_name=\"final_best_model\"\n","    )\n","\n","    # Final model trainer\n","    trainer = Trainer(\n","        model_init=model_init,\n","        args=final_training_args,\n","        train_dataset=tokenized_train,\n","        eval_dataset=tokenized_eval,\n","        tokenizer=tokenizer,\n","        compute_metrics=compute_metrics\n","    )\n","\n","    trainer.train()\n","\n","    # Save best model\n","    best_model_path = f\"/content/drive/MyDrive/Colab Notebooks/{project_name}/best_model\"\n","    trainer.save_model(best_model_path)\n","    print(f\"Best model saved to {best_model_path}\")\n","\n","    wandb.finish()\n","    return best_model_path, best_run\n"],"metadata":{"id":"k3P_OZhW4b-W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**First Model**\n","\n","twitter-roberta-base-sentiment\n","\n","the function above save the best model automatically"],"metadata":{"id":"tqZ1N_6cj6mj"}},{"cell_type":"code","source":["# Attempt to remove the Optuna database file before starting the study\n","!rm -f \"/content/drive/MyDrive/Colab Notebooks/roberta_sentiment_5_cutted_data/optuna_trials.db\"\n","\n","best_model_path, best_roberta_run = train_with_optuna_wandb(\n","    project_name=\"roberta_sentiment_cutted_data_exc5\",\n","    model_name=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    num_labels=5,\n","    n_trials=5,\n","    num_train_epochs=6\n",")"],"metadata":{"id":"1bel9PMt4i_r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Second Model**\n","\n","distilbert-base-uncased-finetuned-sst-2-english\n","\n","the function above save the best model automatically"],"metadata":{"id":"deJh2tAsj7Ul"}},{"cell_type":"code","source":["!rm -f \"/content/drive/MyDrive/Colab Notebooks/distilbert_sentiment_5_cutted_data/optuna_trials.db\"\n","\n","best_model_distil_path, best_distil_run = train_with_optuna_wandb(\n","    project_name=\"distilbert_sentiment_5_cutted_data_exc5\",\n","    model_name=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    num_labels=5,\n","    n_trials=5,\n","    num_train_epochs=6\n",")"],"metadata":{"id":"jEKluamo4jv-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<center><h1>END</h1></center>\n"],"metadata":{"id":"5Yx97wGLkqBJ"}}]}