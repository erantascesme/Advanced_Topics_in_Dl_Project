{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><br><font size=6>Final Project</font><br>\n",
        "<font size=5>Advanced Topics in Deep Learning</font><br>\n",
        "<b><font size=4>Part B</font></b>\n",
        "<br><font size=4>Load Final Models</font><br><br>\n",
        "Authors: Ido Rappaport & Eran Tascesme\n",
        "</font></center>\n",
        "\n",
        "**Submission Details:**\n",
        "<font size=2>\n",
        "<br>Ido Rappaport, ID: 322891623\n",
        "<br>Eran Tascesme , ID: 205708720 </font>\n"
      ],
      "metadata": {
        "id": "3WMGxrvUtebT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**\n",
        "\n",
        "❗Note the versions of the packages, we have included information in requirements.txt❗"
      ],
      "metadata": {
        "id": "xGFqouBeRlMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "# Data handling and visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Machine learning and deep learning\n",
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn, optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import (\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    matthews_corrcoef,\n",
        "    normalized_mutual_info_score,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "\n",
        "# Hugging Face Transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        "    set_seed,\n",
        "    TrainerCallback,\n",
        "    TrainerState,\n",
        "    TrainerControl,\n",
        "    DataCollatorWithPadding,\n",
        "    RobertaForSequenceClassification,\n",
        "    MarianMTModel,\n",
        "    MarianTokenizer\n",
        ")\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "import evaluate\n",
        "\n",
        "# Other libraries\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Filter warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "54mTHFBatr6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load CSV Files**\n"
      ],
      "metadata": {
        "id": "wl6qqnn4t08m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv(\"/data/test_clean.csv\", encoding=\"ISO-8859-1\")\n",
        "path_dir = \"/final_models/\""
      ],
      "metadata": {
        "id": "jP_yVbtqt0ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Models**\n",
        "\n",
        "Load the models and create a list containing all of them. Each model type has its own specific loading function."
      ],
      "metadata": {
        "id": "BbTCmSkBR8pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_list = []"
      ],
      "metadata": {
        "id": "qtMCuQH5SAa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_base_model(model_name, base_weights_path):\n",
        "\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\n",
        "      model_name, num_labels=5, ignore_mismatched_sizes=True\n",
        "  )\n",
        "  state_dict = torch.load(base_weights_path, map_location=\"cpu\")\n",
        "  model.load_state_dict(state_dict)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "  return model, tokenizer\n",
        "\n",
        "\n",
        "def load_compressed_model(save_model_path, device=\"cpu\"):\n",
        "    model_path = os.path.join(save_model_path, \"model.pt\")\n",
        "    model = torch.load(model_path, map_location=device, weights_only=False)\n",
        "    model.eval()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(save_model_path)\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def load_quantized_model(model_name, quantized_model_path):\n",
        "  q_state_path = os.path.join(quantized_model_path, \"model.pt\")\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "  # Rebuild the same base architecture\n",
        "  loaded_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "      model_name, num_labels=5, ignore_mismatched_sizes=True\n",
        "  )\n",
        "\n",
        "  # Apply the same dynamic quantization to convert Linear -> quantized Linear\n",
        "  loaded_model = torch.quantization.quantize_dynamic(\n",
        "      loaded_model, {nn.Linear}, dtype=torch.qint8\n",
        "  )\n",
        "\n",
        "  # Now load the quantized weights (keys will match)\n",
        "  loaded_model.load_state_dict(torch.load(q_state_path, map_location=\"cpu\"))\n",
        "  loaded_model.eval()\n",
        "\n",
        "  return loaded_model, tokenizer"
      ],
      "metadata": {
        "id": "Et3zl8yqSDka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Base Models**\n",
        "\n",
        "The models who trained by exc4 and exc5 without compression (kivutz)"
      ],
      "metadata": {
        "id": "j8r4ybVoSGvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#roberta\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "\n",
        "base_weights_path = \"/final_models/roberta_sentiment_exc4_weights.pt\"\n",
        "model, tokenizer = load_base_model(model_name, base_weights_path)\n",
        "model_list.append((\"roberta_sentiment_exc4\", model, tokenizer))\n",
        "\n",
        "base_weights_path = \"/final_models/roberta_sentiment_weights.pt\"\n",
        "model, tokenizer = load_base_model(model_name, base_weights_path)\n",
        "model_list.append((\"roberta_sentiment_exc5\", model, tokenizer))\n",
        "\n",
        "# distilbert\n",
        "model_name = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "base_weights_path = \"/final_models/distilbert_exc4_weights.pt\"\n",
        "model, tokenizer = load_base_model(model_name, base_weights_path)\n",
        "model_list.append((\"distilbert_exc4\", model, tokenizer))\n",
        "\n",
        "base_weights_path = \"/final_models/distilbert_weights.pt\"\n",
        "model, tokenizer = load_base_model(model_name, base_weights_path)\n",
        "model_list.append((\"distilbert_exc5\", model, tokenizer))"
      ],
      "metadata": {
        "id": "-5-oYRjbSERD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pruned & Distilled Models**"
      ],
      "metadata": {
        "id": "W6iZ_TPMSKtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models_pathes = [\"roberta_exc4_pruned\", \"roberta_pruned\", \"distilbert_exc4_pruned\", \"distilbert_pruned\",\n",
        "                 \"distilroberta_exc4-base\", \"distilroberta-base\", \"tinybert_exc4\", \"tinybert\"]\n",
        "\n",
        "\n",
        "for model_path in models_pathes:\n",
        "    save_model_path = os.path.join(path_dir, model_path)\n",
        "    model, tokenizer = load_compressed_model(save_model_path, device=\"cpu\")\n",
        "\n",
        "    model_list.append((model_path, model, tokenizer))"
      ],
      "metadata": {
        "id": "2dvplpWxSL94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantized Models**"
      ],
      "metadata": {
        "id": "AIF65gxTSMU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#roberta\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "models_pathes = [\"roberta_sentiment_exc4_quantized\", \"roberta_sentiment_quantized\"]\n",
        "\n",
        "for model_path in models_pathes:\n",
        "  quantized_model_path = os.path.join(path_dir, model_path)\n",
        "  model, tokenizer = load_quantized_model(model_name, quantized_model_path)\n",
        "\n",
        "  model_list.append((model_path, model, tokenizer))\n",
        "\n",
        "#distilbert\n",
        "model_name = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "models_pathes = [\"distilbert_exc4_quantized\", \"distilbert_quantized\"]\n",
        "\n",
        "for model_path in models_pathes:\n",
        "  quantized_model_path = os.path.join(path_dir, model_path)\n",
        "  model, tokenizer = load_quantized_model(model_name, quantized_model_path)\n",
        "\n",
        "  model_list.append((model_path, model, tokenizer))"
      ],
      "metadata": {
        "id": "LbdVBpBOSMo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**\n",
        "\n",
        "Evaluate all models and calculate a final score for each."
      ],
      "metadata": {
        "id": "9Plho-VSSRDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_metrics(model, tokenizer, test_data, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    \"\"\"Compute metrics for a single model on test data.\"\"\"\n",
        "    is_quantized = any(\n",
        "        isinstance(m, nn.quantized.dynamic.Linear) or isinstance(m, nn.quantized.Linear)\n",
        "        for m in model.modules()\n",
        "    )\n",
        "    if is_quantized:\n",
        "      device = \"cpu\"\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    texts = test_data['text'].tolist()\n",
        "    labels = torch.tensor(test_data['label'].tolist()).to(device)\n",
        "\n",
        "    start_time = time.time()\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = [str(t) for t in texts[i:i+batch_size]]\n",
        "            encodings = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "            outputs = model(**encodings)\n",
        "\n",
        "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "    runtime = time.time() - start_time\n",
        "\n",
        "    accuracy = accuracy_score(labels.cpu().numpy(), all_preds)\n",
        "    f1 = f1_score(labels.cpu().numpy(), all_preds, average='macro')\n",
        "\n",
        "    mcc = matthews_corrcoef(labels.cpu().numpy(), all_preds)\n",
        "    nit = normalized_mutual_info_score(labels.cpu().numpy(), all_preds)\n",
        "\n",
        "    conf_matrix = confusion_matrix(labels.cpu().numpy(), all_preds)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    nonzero_params = sum(torch.count_nonzero(p).item() for p in model.parameters())\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'mcc': mcc,\n",
        "        'nit': nit,\n",
        "        'conf_matrix': conf_matrix,\n",
        "        'runtime_sec': runtime,\n",
        "        'total_params': total_params,\n",
        "        'nonzero_params': nonzero_params\n",
        "    }\n",
        "\n",
        "def evaluate_and_score_models(model_list, test_data, weights=None, batch_size=32):\n",
        "    \"\"\"\n",
        "    Evaluate multiple HuggingFace models and compute a relative weighted score.\n",
        "\n",
        "    Args:\n",
        "        models: list of (name, model, tokenizer)\n",
        "        test_data: pd.DataFrame with 'text' and 'label'\n",
        "        weights: dict with weights for metrics\n",
        "        batch_size: evaluation batch size\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame with metrics and final weighted score\n",
        "    \"\"\"\n",
        "    if weights is None:\n",
        "        weights = {'accuracy': 0.4, 'mcc': 0.2, 'nit': 0.2,'runtime': 0.1, 'params': 0.05, 'nonzero_params': 0.05}\n",
        "\n",
        "    all_metrics = {}\n",
        "\n",
        "    # Step 1: compute metrics for all models\n",
        "    for name, model, tokenizer in model_list:\n",
        "      print(f\"Evaluating {name}...\")\n",
        "      try:\n",
        "          metrics = evaluate_model_metrics(model, tokenizer, test_data, batch_size=batch_size)\n",
        "          all_metrics[name] = metrics\n",
        "      except Exception as e:\n",
        "          print(f\"Error evaluating {name}: {e}\")\n",
        "\n",
        "    df = pd.DataFrame(all_metrics).T\n",
        "\n",
        "    # Step 2: min-max scale each metric (higher is better for final score)\n",
        "    df_scaled = df.copy()\n",
        "\n",
        "    # For metrics where lower is better (runtime, total_params, nonzero_params)\n",
        "    for col in ['runtime_sec', 'total_params', 'nonzero_params']:\n",
        "        col_normalization = col + \"_norm\"\n",
        "        df_scaled[col_normalization] = 1 / ((df[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-8) + 0.5)\n",
        "\n",
        "    # Step 3: compute final weighted score\n",
        "    df_scaled['final_score'] = (\n",
        "        weights['accuracy'] * df_scaled['accuracy'] +\n",
        "        weights['mcc'] * df_scaled['mcc'] +\n",
        "        weights['nit'] * df_scaled['nit'] +\n",
        "        weights['runtime'] * df_scaled['runtime_sec_norm'] +\n",
        "        weights['params'] * df_scaled['total_params_norm'] +\n",
        "        weights['nonzero_params'] * df_scaled['nonzero_params_norm']\n",
        "    )\n",
        "\n",
        "    # Sort by final score\n",
        "    df_scaled = df_scaled.sort_values(by='final_score', ascending=False)\n",
        "\n",
        "    return df_scaled\n"
      ],
      "metadata": {
        "id": "T0k6lhmJSRVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_df = evaluate_and_score_models(model_list, test_data)\n",
        "print(evaluation_df.columns)"
      ],
      "metadata": {
        "id": "uBJ-X-A8Se6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_df[['accuracy', 'f1', 'mcc', 'nit', 'runtime_sec', 'params', 'final_score']]"
      ],
      "metadata": {
        "id": "PalZZYqiSgzK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}