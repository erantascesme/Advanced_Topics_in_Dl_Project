{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WMGxrvUtebT"
   },
   "source": [
    "<center><br><font size=6>Final Project</font><br>\n",
    "<font size=5>Advanced Topics in Deep Learning</font><br>\n",
    "<b><font size=4>Part B</font></b>\n",
    "<br><font size=4>Load Final Models</font><br><br>\n",
    "Authors: Ido Rappaport & Eran Tascesme\n",
    "</font></center>\n",
    "\n",
    "**Submission Details:**\n",
    "<font size=2>\n",
    "<br>Ido Rappaport, ID: 322891623\n",
    "<br>Eran Tascesme , ID: 205708720 </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGFqouBeRlMB"
   },
   "source": [
    "**Import libraries**\n",
    "\n",
    "❗Note the versions of the packages, we have included information in requirements.txt❗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "54mTHFBatr6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eran\\anaconda3\\envs\\DeepL\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "C:\\Users\\Eran\\anaconda3\\envs\\DeepL\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Data handling and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning and deep learning\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    normalized_mutual_info_score,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from peft import PeftModel\n",
    "\n",
    "# Filter warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wl6qqnn4t08m"
   },
   "source": [
    "**Load CSV Files**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jP_yVbtqt0ge"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/test_clean.csv\", encoding=\"ISO-8859-1\")\n",
    "path_dir = \"final_models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbTCmSkBR8pG"
   },
   "source": [
    "**Load Models**\n",
    "\n",
    "Load the models and create a list containing all of them. Each model type has its own specific loading function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qtMCuQH5SAa6"
   },
   "outputs": [],
   "source": [
    "model_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Et3zl8yqSDka"
   },
   "outputs": [],
   "source": [
    "# function that load the Base Models who trained by exc4 and exc5 without compression (kivutz)\n",
    "def load_base_model(model_name, base_weights_path):\n",
    "\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(\n",
    "      model_name, num_labels=5, ignore_mismatched_sizes=True\n",
    "  )\n",
    "  state_dict = torch.load(base_weights_path, map_location=\"cpu\")\n",
    "  model.load_state_dict(state_dict)\n",
    "\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  return model, tokenizer\n",
    "\n",
    "\n",
    "# function that load the pruned and distilled models\n",
    "def load_compressed_model(save_model_path, device=\"cpu\"):\n",
    "    model_path = os.path.join(save_model_path, \"model.pt\")\n",
    "    model = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# function that load the quantized models\n",
    "def load_quantized_model(model_name, quantized_model_path):\n",
    "  q_state_path = os.path.join(quantized_model_path, \"model.pt\")\n",
    "\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  # Rebuild the same base architecture\n",
    "  loaded_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "      model_name, num_labels=5, ignore_mismatched_sizes=True\n",
    "  )\n",
    "\n",
    "  # Apply the same dynamic quantization to convert Linear -> quantized Linear\n",
    "  loaded_model = torch.quantization.quantize_dynamic(\n",
    "      loaded_model, {nn.Linear}, dtype=torch.qint8\n",
    "  )\n",
    "\n",
    "  # Now load the quantized weights (keys will match)\n",
    "  loaded_model.load_state_dict(torch.load(q_state_path, map_location=\"cpu\"))\n",
    "  loaded_model.eval()\n",
    "\n",
    "  return loaded_model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8r4ybVoSGvP"
   },
   "source": [
    "**Base Models**\n",
    "\n",
    "The models who trained by exc4 and exc5 without compression (kivutz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5-oYRjbSERD",
    "outputId": "d5ace12a-0737-4888-c015-75f292d485f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#roberta\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "base_weights_path = \"final_models/roberta_sentiment_exc4_weights.pt\"\n",
    "model, tokenizer = load_base_model(model_name, base_weights_path)\n",
    "model_list.append((\"roberta_sentiment_exc4\", model, tokenizer))\n",
    "\n",
    "base_weights_path = \"final_models/roberta_sentiment_weights.pt\"\n",
    "model, tokenizer = load_base_model(model_name, base_weights_path)\n",
    "model_list.append((\"roberta_sentiment_exc5\", model, tokenizer))\n",
    "\n",
    "# distilbert\n",
    "model_name = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "base_weights_path = \"final_models/distilbert_exc4_weights.pt\"\n",
    "model, tokenizer = load_base_model(model_name, base_weights_path)\n",
    "model_list.append((\"distilbert_exc4\", model, tokenizer))\n",
    "\n",
    "base_weights_path = \"final_models/distilbert_weights.pt\"\n",
    "model, tokenizer = load_base_model(model_name, base_weights_path)\n",
    "model_list.append((\"distilbert_exc5\", model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6iZ_TPMSKtf"
   },
   "source": [
    "**Pruned & Distilled Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2dvplpWxSL94"
   },
   "outputs": [],
   "source": [
    "models_pathes = [\"roberta_exc4_pruned\", \"roberta_pruned\", \"distilbert_exc4_pruned\", \"distilbert_pruned\",\n",
    "                 \"distilroberta_exc4-base\", \"distilroberta-base\", \"tinybert_exc4\", \"tinybert\"]\n",
    "\n",
    "\n",
    "for model_path in models_pathes:\n",
    "    save_model_path = os.path.join(path_dir, model_path)\n",
    "    model, tokenizer = load_compressed_model(save_model_path, device=\"cpu\")\n",
    "\n",
    "    model_list.append((model_path, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIF65gxTSMU0"
   },
   "source": [
    "**Quantized Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LbdVBpBOSMo9",
    "outputId": "bf9bb326-458e-4714-e0a8-93767cf584fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#roberta\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "models_pathes = [\"roberta_sentiment_exc4_quantized\", \"roberta_sentiment_quantized\"]\n",
    "\n",
    "for model_path in models_pathes:\n",
    "  quantized_model_path = os.path.join(path_dir, model_path)\n",
    "  model, tokenizer = load_quantized_model(model_name, quantized_model_path)\n",
    "\n",
    "  model_list.append((model_path, model, tokenizer))\n",
    "\n",
    "#distilbert\n",
    "model_name = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "models_pathes = [\"distilbert_exc4_quantized\", \"distilbert_quantized\"]\n",
    "\n",
    "for model_path in models_pathes:\n",
    "  quantized_model_path = os.path.join(path_dir, model_path)\n",
    "  model, tokenizer = load_quantized_model(model_name, quantized_model_path)\n",
    "\n",
    "  model_list.append((model_path, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yg5ypT99d0Up"
   },
   "source": [
    "**LoRA Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bey1qE7ed0Dk",
    "outputId": "adcbf1d3-4690-466c-a1cd-e00bf188bbfd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#roberta\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "lora_list = [\"roberta_sentiment_exc4_LoRA\", \"roberta_sentiment_exc5_LoRA\"]\n",
    "\n",
    "for model_path in lora_list:\n",
    "    save_model_path = os.path.join(path_dir, model_path)\n",
    "\n",
    "    base_model_reload = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, ignore_mismatched_sizes=True)\n",
    "    lora_model_reload = PeftModel.from_pretrained(base_model_reload, save_model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_model_path)\n",
    "\n",
    "    model_list.append((model_path, lora_model_reload, tokenizer))\n",
    "\n",
    "\n",
    "#distilbert\n",
    "lora_list = [\"distilbert_exc4_LoRA\", \"distilbert_exc5_LoRA\"]\n",
    "model_name = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "for model_path in lora_list:\n",
    "    save_model_path = os.path.join(path_dir, model_path)\n",
    "\n",
    "    base_model_reload = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, ignore_mismatched_sizes=True)\n",
    "    lora_model_reload = PeftModel.from_pretrained(base_model_reload, save_model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_model_path)\n",
    "\n",
    "    model_list.append((model_path, lora_model_reload, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Plho-VSSRDc"
   },
   "source": [
    "**Evaluation**\n",
    "\n",
    "Evaluate all models and calculate a final score for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "T0k6lhmJSRVu"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_metrics(model, tokenizer, test_data, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"Compute metrics for a single model on test data.\"\"\"\n",
    "    # check if the model is quantized and move it to the cpu/gpu .\n",
    "    is_quantized = any(\n",
    "        isinstance(m, nn.quantized.dynamic.Linear) or isinstance(m, nn.quantized.Linear)\n",
    "        for m in model.modules()\n",
    "    )\n",
    "    if is_quantized:\n",
    "      device = \"cpu\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # store the texts and labels\n",
    "    texts = test_data['text'].tolist()\n",
    "    labels = torch.tensor(test_data['label'].tolist()).to(device)\n",
    "\n",
    "    # start calculate the metrics. time, acc, f1, num_parmas and more.\n",
    "    start_time = time.time()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = [str(t) for t in texts[i:i+batch_size]]\n",
    "            encodings = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "            outputs = model(**encodings)\n",
    "\n",
    "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())    # store the preds\n",
    "    runtime = time.time() - start_time\n",
    "\n",
    "    accuracy = accuracy_score(labels.cpu().numpy(), all_preds)\n",
    "    f1 = f1_score(labels.cpu().numpy(), all_preds, average='macro')\n",
    "\n",
    "    mcc = matthews_corrcoef(labels.cpu().numpy(), all_preds)\n",
    "    nit = normalized_mutual_info_score(labels.cpu().numpy(), all_preds)\n",
    "\n",
    "    conf_matrix = confusion_matrix(labels.cpu().numpy(), all_preds)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    nonzero_params = sum(torch.count_nonzero(p).item() for p in model.parameters())\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'mcc': mcc,\n",
    "        'nit': nit,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'runtime_sec': runtime,\n",
    "        'total_params': total_params,\n",
    "        'nonzero_params': nonzero_params\n",
    "    }\n",
    "\n",
    "def evaluate_and_score_models(model_list, test_data, weights=None, batch_size=32):\n",
    "    \"\"\"\n",
    "    Evaluate multiple HuggingFace models and compute a relative weighted score.\n",
    "\n",
    "    Args:\n",
    "        models: list of (name, model, tokenizer)\n",
    "        test_data: pd.DataFrame with 'text' and 'label'\n",
    "        weights: dict with weights for metrics\n",
    "        batch_size: evaluation batch size\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame with metrics and final weighted score\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = {'accuracy': 0.4, 'mcc': 0.2, 'nit': 0.2,'runtime': 0.1, 'params': 0.05, 'nonzero_params': 0.05}\n",
    "\n",
    "    all_metrics = {}\n",
    "\n",
    "    # Step 1: compute metrics for all models\n",
    "    for name, model, tokenizer in model_list:\n",
    "      print(f\"Evaluating {name}...\")\n",
    "      try:\n",
    "          metrics = evaluate_model_metrics(model, tokenizer, test_data, batch_size=batch_size)\n",
    "          all_metrics[name] = metrics\n",
    "      except Exception as e:\n",
    "          print(f\"Error evaluating {name}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(all_metrics).T\n",
    "\n",
    "    # Step 2: min-max scale each metric (higher is better for final score)\n",
    "    df_scaled = df.copy()\n",
    "\n",
    "    # For metrics where lower is better (runtime, total_params, nonzero_params)\n",
    "    for col in ['runtime_sec', 'total_params', 'nonzero_params']:\n",
    "        col_normalization = col + \"_norm\"\n",
    "        df_scaled[col_normalization] = 1 / ((df[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-8) + 0.5)\n",
    "\n",
    "    # Step 3: compute final weighted score\n",
    "    df_scaled['final_score'] = (\n",
    "        weights['accuracy'] * df_scaled['accuracy'] +\n",
    "        weights['mcc'] * df_scaled['mcc'] +\n",
    "        weights['nit'] * df_scaled['nit'] +\n",
    "        weights['runtime'] * df_scaled['runtime_sec_norm'] +\n",
    "        weights['params'] * df_scaled['total_params_norm'] +\n",
    "        weights['nonzero_params'] * df_scaled['nonzero_params_norm']\n",
    "    )\n",
    "\n",
    "    # Sort by final score\n",
    "    df_scaled = df_scaled.sort_values(by='final_score', ascending=False)\n",
    "\n",
    "    return df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBJ-X-A8Se6L",
    "outputId": "c8bf467b-05e9-438b-8eab-b5d5984ec1b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating roberta_sentiment_exc4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating roberta_sentiment_exc5...\n",
      "Evaluating distilbert_exc4...\n",
      "Evaluating distilbert_exc5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating roberta_exc4_pruned...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating roberta_pruned...\n",
      "Evaluating distilbert_exc4_pruned...\n",
      "Evaluating distilbert_pruned...\n",
      "Evaluating distilroberta_exc4-base...\n",
      "Evaluating distilroberta-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating tinybert_exc4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating tinybert...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating roberta_sentiment_exc4_quantized...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating roberta_sentiment_quantized...\n",
      "Evaluating distilbert_exc4_quantized...\n",
      "Evaluating distilbert_quantized...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating roberta_sentiment_exc4_LoRA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating roberta_sentiment_exc5_LoRA...\n",
      "Evaluating distilbert_exc4_LoRA...\n",
      "Evaluating distilbert_exc5_LoRA...\n",
      "Index(['accuracy', 'f1', 'mcc', 'nit', 'conf_matrix', 'runtime_sec',\n",
      "       'total_params', 'nonzero_params', 'runtime_sec_norm',\n",
      "       'total_params_norm', 'nonzero_params_norm', 'final_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# start evaluation\n",
    "evaluation_df = evaluate_and_score_models(model_list, test_data)\n",
    "print(evaluation_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "PalZZYqiSgzK",
    "outputId": "4e7e9450-af0c-49e2-9e75-2d6605c502f0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>mcc</th>\n",
       "      <th>nit</th>\n",
       "      <th>runtime_sec</th>\n",
       "      <th>total_params</th>\n",
       "      <th>final_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>distilbert_quantized</th>\n",
       "      <td>0.749342</td>\n",
       "      <td>0.759337</td>\n",
       "      <td>0.681289</td>\n",
       "      <td>0.498732</td>\n",
       "      <td>52.92968</td>\n",
       "      <td>23854080</td>\n",
       "      <td>0.83656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta_sentiment_quantized</th>\n",
       "      <td>0.736177</td>\n",
       "      <td>0.74678</td>\n",
       "      <td>0.665713</td>\n",
       "      <td>0.479057</td>\n",
       "      <td>94.811949</td>\n",
       "      <td>39037440</td>\n",
       "      <td>0.757835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilbert_pruned</th>\n",
       "      <td>0.747499</td>\n",
       "      <td>0.757465</td>\n",
       "      <td>0.678261</td>\n",
       "      <td>0.493489</td>\n",
       "      <td>78.844361</td>\n",
       "      <td>66957317</td>\n",
       "      <td>0.752405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilbert_exc5</th>\n",
       "      <td>0.750132</td>\n",
       "      <td>0.76129</td>\n",
       "      <td>0.682747</td>\n",
       "      <td>0.500303</td>\n",
       "      <td>71.512965</td>\n",
       "      <td>66957317</td>\n",
       "      <td>0.75172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilbert_exc4_quantized</th>\n",
       "      <td>0.604529</td>\n",
       "      <td>0.616722</td>\n",
       "      <td>0.497042</td>\n",
       "      <td>0.322598</td>\n",
       "      <td>44.115964</td>\n",
       "      <td>23854080</td>\n",
       "      <td>0.717075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta_sentiment_exc5</th>\n",
       "      <td>0.748289</td>\n",
       "      <td>0.75888</td>\n",
       "      <td>0.681719</td>\n",
       "      <td>0.498854</td>\n",
       "      <td>145.650394</td>\n",
       "      <td>124649477</td>\n",
       "      <td>0.6752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tinybert</th>\n",
       "      <td>0.406793</td>\n",
       "      <td>0.386043</td>\n",
       "      <td>0.316721</td>\n",
       "      <td>0.206525</td>\n",
       "      <td>11.71474</td>\n",
       "      <td>14351813</td>\n",
       "      <td>0.667367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilbert_exc4_pruned</th>\n",
       "      <td>0.61901</td>\n",
       "      <td>0.633854</td>\n",
       "      <td>0.518997</td>\n",
       "      <td>0.350883</td>\n",
       "      <td>73.033131</td>\n",
       "      <td>66957317</td>\n",
       "      <td>0.645127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tinybert_exc4</th>\n",
       "      <td>0.373354</td>\n",
       "      <td>0.331221</td>\n",
       "      <td>0.288318</td>\n",
       "      <td>0.194163</td>\n",
       "      <td>12.61069</td>\n",
       "      <td>14351813</td>\n",
       "      <td>0.643525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilbert_exc4</th>\n",
       "      <td>0.630332</td>\n",
       "      <td>0.642991</td>\n",
       "      <td>0.533828</td>\n",
       "      <td>0.346945</td>\n",
       "      <td>78.176526</td>\n",
       "      <td>66957317</td>\n",
       "      <td>0.638113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta_sentiment_exc4_quantized</th>\n",
       "      <td>0.522643</td>\n",
       "      <td>0.54012</td>\n",
       "      <td>0.394144</td>\n",
       "      <td>0.248381</td>\n",
       "      <td>92.65159</td>\n",
       "      <td>39037440</td>\n",
       "      <td>0.573288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilroberta-base</th>\n",
       "      <td>0.537915</td>\n",
       "      <td>0.544353</td>\n",
       "      <td>0.439142</td>\n",
       "      <td>0.271259</td>\n",
       "      <td>73.741372</td>\n",
       "      <td>82122245</td>\n",
       "      <td>0.557866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilroberta_exc4-base</th>\n",
       "      <td>0.532122</td>\n",
       "      <td>0.53957</td>\n",
       "      <td>0.429216</td>\n",
       "      <td>0.263544</td>\n",
       "      <td>79.319007</td>\n",
       "      <td>82122245</td>\n",
       "      <td>0.547746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta_sentiment_exc4</th>\n",
       "      <td>0.592417</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.48801</td>\n",
       "      <td>0.306697</td>\n",
       "      <td>161.504772</td>\n",
       "      <td>124649477</td>\n",
       "      <td>0.530583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta_pruned</th>\n",
       "      <td>0.548183</td>\n",
       "      <td>0.464587</td>\n",
       "      <td>0.431959</td>\n",
       "      <td>0.38172</td>\n",
       "      <td>146.275552</td>\n",
       "      <td>124649477</td>\n",
       "      <td>0.53027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta_sentiment_exc4_LoRA</th>\n",
       "      <td>0.375197</td>\n",
       "      <td>0.330316</td>\n",
       "      <td>0.201075</td>\n",
       "      <td>0.132575</td>\n",
       "      <td>161.651775</td>\n",
       "      <td>125538826</td>\n",
       "      <td>0.351082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta_exc4_pruned</th>\n",
       "      <td>0.339389</td>\n",
       "      <td>0.273564</td>\n",
       "      <td>0.1888</td>\n",
       "      <td>0.098568</td>\n",
       "      <td>142.446745</td>\n",
       "      <td>124649477</td>\n",
       "      <td>0.34283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilbert_exc4_LoRA</th>\n",
       "      <td>0.28752</td>\n",
       "      <td>0.116906</td>\n",
       "      <td>0.044334</td>\n",
       "      <td>0.033471</td>\n",
       "      <td>78.512503</td>\n",
       "      <td>67699210</td>\n",
       "      <td>0.339443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilbert_exc5_LoRA</th>\n",
       "      <td>0.280147</td>\n",
       "      <td>0.110438</td>\n",
       "      <td>0.028135</td>\n",
       "      <td>0.01953</td>\n",
       "      <td>78.050575</td>\n",
       "      <td>67699210</td>\n",
       "      <td>0.330812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta_sentiment_exc5_LoRA</th>\n",
       "      <td>0.308057</td>\n",
       "      <td>0.194544</td>\n",
       "      <td>0.094354</td>\n",
       "      <td>0.085948</td>\n",
       "      <td>164.845043</td>\n",
       "      <td>125538826</td>\n",
       "      <td>0.292617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  accuracy        f1       mcc       nit  \\\n",
       "distilbert_quantized              0.749342  0.759337  0.681289  0.498732   \n",
       "roberta_sentiment_quantized       0.736177   0.74678  0.665713  0.479057   \n",
       "distilbert_pruned                 0.747499  0.757465  0.678261  0.493489   \n",
       "distilbert_exc5                   0.750132   0.76129  0.682747  0.500303   \n",
       "distilbert_exc4_quantized         0.604529  0.616722  0.497042  0.322598   \n",
       "roberta_sentiment_exc5            0.748289   0.75888  0.681719  0.498854   \n",
       "tinybert                          0.406793  0.386043  0.316721  0.206525   \n",
       "distilbert_exc4_pruned             0.61901  0.633854  0.518997  0.350883   \n",
       "tinybert_exc4                     0.373354  0.331221  0.288318  0.194163   \n",
       "distilbert_exc4                   0.630332  0.642991  0.533828  0.346945   \n",
       "roberta_sentiment_exc4_quantized  0.522643   0.54012  0.394144  0.248381   \n",
       "distilroberta-base                0.537915  0.544353  0.439142  0.271259   \n",
       "distilroberta_exc4-base           0.532122   0.53957  0.429216  0.263544   \n",
       "roberta_sentiment_exc4            0.592417    0.6065   0.48801  0.306697   \n",
       "roberta_pruned                    0.548183  0.464587  0.431959   0.38172   \n",
       "roberta_sentiment_exc4_LoRA       0.375197  0.330316  0.201075  0.132575   \n",
       "roberta_exc4_pruned               0.339389  0.273564    0.1888  0.098568   \n",
       "distilbert_exc4_LoRA               0.28752  0.116906  0.044334  0.033471   \n",
       "distilbert_exc5_LoRA              0.280147  0.110438  0.028135   0.01953   \n",
       "roberta_sentiment_exc5_LoRA       0.308057  0.194544  0.094354  0.085948   \n",
       "\n",
       "                                 runtime_sec total_params final_score  \n",
       "distilbert_quantized                52.92968     23854080     0.83656  \n",
       "roberta_sentiment_quantized        94.811949     39037440    0.757835  \n",
       "distilbert_pruned                  78.844361     66957317    0.752405  \n",
       "distilbert_exc5                    71.512965     66957317     0.75172  \n",
       "distilbert_exc4_quantized          44.115964     23854080    0.717075  \n",
       "roberta_sentiment_exc5            145.650394    124649477      0.6752  \n",
       "tinybert                            11.71474     14351813    0.667367  \n",
       "distilbert_exc4_pruned             73.033131     66957317    0.645127  \n",
       "tinybert_exc4                       12.61069     14351813    0.643525  \n",
       "distilbert_exc4                    78.176526     66957317    0.638113  \n",
       "roberta_sentiment_exc4_quantized    92.65159     39037440    0.573288  \n",
       "distilroberta-base                 73.741372     82122245    0.557866  \n",
       "distilroberta_exc4-base            79.319007     82122245    0.547746  \n",
       "roberta_sentiment_exc4            161.504772    124649477    0.530583  \n",
       "roberta_pruned                    146.275552    124649477     0.53027  \n",
       "roberta_sentiment_exc4_LoRA       161.651775    125538826    0.351082  \n",
       "roberta_exc4_pruned               142.446745    124649477     0.34283  \n",
       "distilbert_exc4_LoRA               78.512503     67699210    0.339443  \n",
       "distilbert_exc5_LoRA               78.050575     67699210    0.330812  \n",
       "roberta_sentiment_exc5_LoRA       164.845043    125538826    0.292617  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df[['accuracy', 'f1', 'mcc', 'nit', 'runtime_sec', 'total_params', 'final_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2YLn2-884Dr"
   },
   "source": [
    "<center><h1>END</h1></center>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Deep)",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
