{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIiX_xdavy_J"
   },
   "source": [
    "<center><br><font size=6>Final Project</font><br>\n",
    "<font size=5>Advanced Topics in Deep Learning</font><br>\n",
    "<b><font size=4>Part B</font></b>\n",
    "<br><font size=4>Models Selection</font><br><br>\n",
    "Authors: Ido Rappaport & Eran Tascesme\n",
    "</font></center>\n",
    "\n",
    "**Submission Details:**\n",
    "<font size=2>\n",
    "<br>Ido Rappaport, ID: 322891623\n",
    "<br>Eran Tascesme , ID: 205708720 </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfSLlM4wkgbQ"
   },
   "source": [
    "**Import libraries**\n",
    "\n",
    "❗Note the versions of the packages, we have included information in requirements.txt❗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxHv15-46Dab"
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "# Data handling and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "from gensim import corpora, models\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Machine learning and deep learning\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    normalized_mutual_info_score,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    "    TrainerCallback,\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    "    DataCollatorWithPadding,\n",
    "    RobertaForSequenceClassification,\n",
    "    MarianMTModel,\n",
    "    MarianTokenizer\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "\n",
    "# Other libraries\n",
    "import optuna\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Filter warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK resources\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVtTlJSCnBXv"
   },
   "source": [
    "** `evaluate_and_score_models`**\n",
    "\n",
    "This function is designed to evaluate a list of models on a given test dataset and compute a relative weighted score based on various metrics.\n",
    "\n",
    "**Purpose:**\n",
    "To compare the performance of multiple models systematically by evaluating them on the same test data and providing a combined weighted score.\n",
    "\n",
    "**Arguments:**\n",
    "- `model_list`: A list of tuples, where each tuple contains (name, model, tokenizer) for a model to be evaluated.\n",
    "- `test_data`: A pandas DataFrame containing the test data with 'text' and 'label' columns.\n",
    "- `weights`: A dictionary containing the weights for each metric used in the final score calculation (defaults to a predefined dictionary if None).\n",
    "- `batch_size`: The batch size to use during evaluation (default is 32).\n",
    "\n",
    "**Functionality:**\n",
    "1. Iterates through the provided list of models.\n",
    "2. For each model, it calls the `evaluate_model_metrics` function to compute various performance metrics (accuracy, F1, MCC, NIT, confusion matrix, runtime, total parameters, non-zero parameters).\n",
    "3. Stores the computed metrics for each model.\n",
    "4. Creates a pandas DataFrame from the collected metrics.\n",
    "5. Normalizes the metrics where lower values are better (runtime, total parameters, non-zero parameters) by taking the inverse of their min-max scaled values.\n",
    "6. Computes a final weighted score for each model based on the specified weights and the scaled metrics.\n",
    "7. Sorts the DataFrame by the final weighted score in descending order.\n",
    "8. Returns the DataFrame containing the metrics and the final weighted score for each model.\n",
    "\n",
    "** `evaluate_model_metrics`**\n",
    "\n",
    "This function is designed to evaluate a single Hugging Face model on a given test dataset and compute various performance metrics.\n",
    "\n",
    "1. Calculates the following metrics:\n",
    "    - **Accuracy:** The proportion of correctly classified instances.\n",
    "    - **F1 Score (macro):** The harmonic mean of precision and recall, calculated independently for each class and then averaged.\n",
    "    - **Matthews Correlation Coefficient (MCC):** A measure of the quality of binary and multiclass classifications.\n",
    "    - **Normalized Mutual Information (NMI):** A measure of the agreement between the true labels and the predicted labels.\n",
    "    - **Confusion Matrix:** A table showing the counts of true positive, true negative, false positive, and false negative predictions.\n",
    "    - **Runtime:** The time taken to evaluate the model on the entire test dataset.\n",
    "    - **Total Parameters:** The total number of parameters in the model.\n",
    "    - **Non-zero Parameters:** The number of non-zero parameters in the model (useful for evaluating sparsity).\n",
    "2. Returns a dictionary containing all the computed metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "410tFDg-j_Ii"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_metrics(model, tokenizer, test_data, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"Compute metrics for a single model on test data.\"\"\"\n",
    "    is_quantized = any(\n",
    "        isinstance(m, nn.quantized.dynamic.Linear) or isinstance(m, nn.quantized.Linear)\n",
    "        for m in model.modules()\n",
    "    )\n",
    "    if is_quantized:\n",
    "      device = \"cpu\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    texts = test_data['text'].tolist()\n",
    "    labels = torch.tensor(test_data['label'].tolist()).to(device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = [str(t) for t in texts[i:i+batch_size]]\n",
    "            encodings = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "            outputs = model(**encodings)\n",
    "\n",
    "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    runtime = time.time() - start_time\n",
    "\n",
    "    accuracy = accuracy_score(labels.cpu().numpy(), all_preds)\n",
    "    f1 = f1_score(labels.cpu().numpy(), all_preds, average='macro')\n",
    "\n",
    "    mcc = matthews_corrcoef(labels.cpu().numpy(), all_preds)\n",
    "    nit = normalized_mutual_info_score(labels.cpu().numpy(), all_preds)\n",
    "\n",
    "    conf_matrix = confusion_matrix(labels.cpu().numpy(), all_preds)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    nonzero_params = sum(torch.count_nonzero(p).item() for p in model.parameters())\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'mcc': mcc,\n",
    "        'nit': nit,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'runtime_sec': runtime,\n",
    "        'total_params': total_params,\n",
    "        'nonzero_params': nonzero_params\n",
    "    }\n",
    "\n",
    "def evaluate_and_score_models(model_list, test_data, weights=None, batch_size=32):\n",
    "    \"\"\"\n",
    "    Evaluate multiple HuggingFace models and compute a relative weighted score.\n",
    "\n",
    "    Args:\n",
    "        models: list of (name, model, tokenizer)\n",
    "        test_data: pd.DataFrame with 'text' and 'label'\n",
    "        weights: dict with weights for metrics\n",
    "        batch_size: evaluation batch size\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame with metrics and final weighted score\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = {'accuracy': 0.4, 'mcc': 0.2, 'nit': 0.2,'runtime': 0.1, 'params': 0.05, 'nonzero_params': 0.05}\n",
    "\n",
    "    all_metrics = {}\n",
    "\n",
    "    # Step 1: compute metrics for all models\n",
    "    for name, model, tokenizer in model_list:\n",
    "      print(f\"Evaluating {name}...\")\n",
    "      try:\n",
    "          metrics = evaluate_model_metrics(model, tokenizer, test_data, batch_size=batch_size)\n",
    "          all_metrics[name] = metrics\n",
    "      except Exception as e:\n",
    "          print(f\"Error evaluating {name}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(all_metrics).T\n",
    "\n",
    "    # Step 2: min-max scale each metric (higher is better for final score)\n",
    "    df_scaled = df.copy()\n",
    "\n",
    "    # For metrics where lower is better (runtime, total_params, nonzero_params)\n",
    "    for col in ['runtime_sec', 'total_params', 'nonzero_params']:\n",
    "        col_normalization = col + \"_norm\"\n",
    "        df_scaled[col_normalization] = 1 / ((df[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-8) + 0.5)\n",
    "\n",
    "    # Step 3: compute final weighted score\n",
    "    df_scaled['final_score'] = (\n",
    "        weights['accuracy'] * df_scaled['accuracy'] +\n",
    "        weights['mcc'] * df_scaled['mcc'] +\n",
    "        weights['nit'] * df_scaled['nit'] +\n",
    "        weights['runtime'] * df_scaled['runtime_sec_norm'] +\n",
    "        weights['params'] * df_scaled['total_params_norm'] +\n",
    "        weights['nonzero_params'] * df_scaled['nonzero_params_norm']\n",
    "    )\n",
    "\n",
    "    # Sort by final score\n",
    "    df_scaled = df_scaled.sort_values(by='final_score', ascending=False)\n",
    "\n",
    "    return df_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8x68HdIqe5j"
   },
   "source": [
    "**Load CSV Files**\n",
    "\n",
    "Based on the results, the best-performing models were those trained on the clean, truncated dataset after augmentation. Therefore, we will proceed using this dataset and these specific models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6F4RInuQaMsu"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/test_clean.csv\", encoding=\"ISO-8859-1\")\n",
    "path_dir = \"final_models/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rsMHnELqnsm"
   },
   "source": [
    "**Load Models**\n",
    "\n",
    "In this section, we will load the models and create a list containing all of them. Each model type has its own specific loading function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCKo4ZxLlCZx"
   },
   "outputs": [],
   "source": [
    "model_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1D6qY9ZXN-f"
   },
   "outputs": [],
   "source": [
    "def load_base_model(model_name, base_weights_path):\n",
    "\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(\n",
    "      model_name, num_labels=5, ignore_mismatched_sizes=True\n",
    "  )\n",
    "  state_dict = torch.load(base_weights_path, map_location=\"cpu\")\n",
    "  model.load_state_dict(state_dict)\n",
    "\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  return model, tokenizer\n",
    "\n",
    "\n",
    "def load_compressed_model(save_model_path, device=\"cpu\"):\n",
    "    model_path = os.path.join(save_model_path, \"model.pt\")\n",
    "    model = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_quantized_model(model_name, quantized_model_path):\n",
    "  q_state_path = os.path.join(quantized_model_path, \"model.pt\")\n",
    "\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  # Rebuild the same base architecture\n",
    "  loaded_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "      model_name, num_labels=5, ignore_mismatched_sizes=True\n",
    "  )\n",
    "\n",
    "  # Apply the same dynamic quantization to convert Linear -> quantized Linear\n",
    "  loaded_model = torch.quantization.quantize_dynamic(\n",
    "      loaded_model, {nn.Linear}, dtype=torch.qint8\n",
    "  )\n",
    "\n",
    "  # Now load the quantized weights (keys will match)\n",
    "  loaded_model.load_state_dict(torch.load(q_state_path, map_location=\"cpu\"))\n",
    "  loaded_model.eval()\n",
    "\n",
    "  return loaded_model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He1Z93ZZXYdY"
   },
   "source": [
    "**Base Models**\n",
    "\n",
    "The models who trained by exc4 and exc5 without compression (kivutz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596,
     "referenced_widgets": [
      "cec5ec66ec844338ab2c8beee6bd4b2b",
      "7d73633d5f614180aed30a85b3436808",
      "a16554ac707046729c71e74019b52e3d",
      "15163e8894d747939ea3d00c2a6b7ac0",
      "d035e6e6d9d943559bae97295868ef54",
      "1286d42881d84048968167abf1f1d8d4",
      "d878bb6c3ab348e88696a2ae05163681",
      "dfb6e2c83d8346b4b3bbbc761c92eb9b",
      "248a9a25106a45d284d5bad50d0e0fd2",
      "ac6ba4417db3445594b510f4a69adfdd",
      "1053e0148434407fa87da15446ee8700",
      "d2d62ede76fe4cc9a028816acc8e12fe",
      "5d0991dd019d4fe0b9052735c7b25b32",
      "c144201d85a3415080cbb5678b75aeb1",
      "83c5745c6c354047b885f980c883f76b",
      "db3223c5c3014a49abff60c091d27d71",
      "87488aba60374c469ab7b8ce165018ee",
      "5d7d5c4b417343c1a45c8d750dc41a40",
      "d1a0a3e062e3481b9f58dd489cd1373d",
      "e4034048651a4551a66536363babf3b8",
      "7db51024206f43bab0f0e03c681a35ac",
      "27aaf3a8954346f5b91b1f907f2acb07",
      "bc9ab58d429543b18eff8fcc52834903",
      "eda1a6cc497d4c058919690e9991bab0",
      "36a1a8cf7d4a438f8fcf52b57bcbf61a",
      "4d3b6c983651474d89abd814754ace49",
      "fe1920240f3148ebb0321e038c0fd8b7",
      "fd2c06402c294bd38673d27901a3220e",
      "a63b45a7417d4d7e90fca347e348dd25",
      "5554d3bc46c24af493c439d291605897",
      "88b8ebbbe4d9480d9475825dbd7c67bb",
      "25b973340d45475b966a560d416a0901",
      "20d2049d8ac147cc80841fbe03504aef",
      "b0087b1bc1004004a8b1ed5d505a8f9e",
      "fe6248fd5b5748e28e8a6600e15bef21",
      "0caa20b29ae3449c90725c343a4ea80c",
      "c15002e3a0c345c9a755253ac99d320a",
      "f104a4b77b0247088dad48d385a78924",
      "a781a1a816df4d109156553814d8f77f",
      "5865bc3106724d9b9162b188ce518933",
      "c5f1af6a433e43348d753bc51f558f31",
      "790153c74b7a4307848a1907a2d154bc",
      "484b98d9ae9546789bd7cf0050cf6aec",
      "8521a4d57a3949e69f6e958a1caf3c0b",
      "f5dfc12f7d7b43c2811c2cc927934ce8",
      "48a8a67db4104f77a3dbf1fc3f17f584",
      "a4ac3d9c1431461f88c5041853a57f1c",
      "f49c264a573042c18eb70fd5124d7a62",
      "5a38ad9f99e3465b9edca145500f38e4",
      "df931a48b5064c3886ad7d5f53d7bf2e",
      "3eafe98c408446d5954ebd5970f3e150",
      "cf5fcace7f23480f941661a0281b7f0a",
      "b3bc4f55db8c4113b68b83a0d99f7ae8",
      "1acd3e94adaa45b69000e1ec626d1995",
      "f957589ab8bb4acfbaf982c850108067"
     ]
    },
    "id": "oVBj-FnZXFJg",
    "outputId": "89196fb0-8211-4005-f9ca-4766d6b140cf"
   },
   "outputs": [],
   "source": [
    "#roberta\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "base_weights_path = \"final_models/roberta_sentiment_exc4_weights.pt\"\n",
    "model, tokenizer = load_base_model(model_name, base_weights_path)\n",
    "model_list.append((\"roberta_sentiment_exc4\", model, tokenizer))\n",
    "\n",
    "base_weights_path = \"final_models/roberta_sentiment_weights.pt\"\n",
    "model, tokenizer = load_base_model(model_name, base_weights_path)\n",
    "model_list.append((\"roberta_sentiment_exc5\", model, tokenizer))\n",
    "\n",
    "# distilbert\n",
    "model_name = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "base_weights_path = \"final_models/distilbert_exc4_weights.pt\"\n",
    "model, tokenizer = load_base_model(model_name, base_weights_path)\n",
    "model_list.append((\"distilbert_exc4\", model, tokenizer))\n",
    "\n",
    "base_weights_path = \"final_models/distilbert_weights.pt\"\n",
    "model, tokenizer = load_base_model(model_name, base_weights_path)\n",
    "model_list.append((\"distilbert_exc5\", model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8K85B_zXUQ1"
   },
   "source": [
    "**Pruned & Distilled Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHDbKXANXWAz"
   },
   "outputs": [],
   "source": [
    "models_pathes = [\"roberta_exc4_pruned\", \"roberta_pruned\", \"distilbert_exc4_pruned\", \"distilbert_pruned\",\n",
    "                 \"distilroberta_exc4-base\", \"distilroberta-base\", \"tinybert_exc4\", \"tinybert\"]\n",
    "\n",
    "\n",
    "for model_path in models_pathes:\n",
    "    save_model_path = os.path.join(path_dir, model_path)\n",
    "    model, tokenizer = load_compressed_model(save_model_path, device=\"cpu\")\n",
    "\n",
    "    model_list.append((model_path, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urMkH3jGrhyF"
   },
   "source": [
    "**Quantized Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxCGbnawpsPf",
    "outputId": "22bb06b6-e1be-49d5-f36b-77cf756a6f8d"
   },
   "outputs": [],
   "source": [
    "#roberta\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "models_pathes = [\"roberta_sentiment_exc4_quantized\", \"roberta_sentiment_quantized\"]\n",
    "\n",
    "for model_path in models_pathes:\n",
    "  quantized_model_path = os.path.join(path_dir, model_path)\n",
    "  model, tokenizer = load_quantized_model(model_name, quantized_model_path)\n",
    "\n",
    "  model_list.append((model_path, model, tokenizer))\n",
    "\n",
    "#distilbert\n",
    "model_name = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "models_pathes = [\"distilbert_exc4_quantized\", \"distilbert_quantized\"]\n",
    "\n",
    "for model_path in models_pathes:\n",
    "  quantized_model_path = os.path.join(path_dir, model_path)\n",
    "  model, tokenizer = load_quantized_model(model_name, quantized_model_path)\n",
    "\n",
    "  model_list.append((model_path, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62XuDtsFKBAT"
   },
   "source": [
    "**LoRA Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOwG2NJ0KAvk",
    "outputId": "4f83350d-3cfc-4e2e-9c5f-149b871af39d"
   },
   "outputs": [],
   "source": [
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "lora_list = [\"roberta_sentiment_exc4_LoRA\", \"roberta_sentiment_exc5_LoRA\"]\n",
    "\n",
    "for model_path in lora_list:\n",
    "    save_model_path = os.path.join(path_dir, model_path)\n",
    "\n",
    "    base_model_reload = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, ignore_mismatched_sizes=True)\n",
    "    lora_model_reload = PeftModel.from_pretrained(base_model_reload, save_model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_model_path)\n",
    "\n",
    "    model_list.append((model_path, lora_model_reload, tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8xVngT7O1ws",
    "outputId": "0cd17ab7-1ef5-4785-bc67-193049af9664"
   },
   "outputs": [],
   "source": [
    "\n",
    "lora_list = [\"distilbert_exc4_LoRA\", \"distilbert_exc5_LoRA\"]\n",
    "model_name = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "for model_path in lora_list:\n",
    "    save_model_path = os.path.join(path_dir, model_path)\n",
    "\n",
    "    base_model_reload = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, ignore_mismatched_sizes=True)\n",
    "    lora_model_reload = PeftModel.from_pretrained(base_model_reload, save_model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_model_path)\n",
    "\n",
    "    model_list.append((model_path, lora_model_reload, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZdRdj_mrmSV"
   },
   "source": [
    "**Evaluation**\n",
    "\n",
    "Evaluate all models and calculate a final score for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmqm1N3GSGlN",
    "outputId": "47dd8586-495b-4964-e52c-020298c0ca3f"
   },
   "outputs": [],
   "source": [
    "evaluation_df = evaluate_and_score_models(model_list, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "_E4ny3cbumkd",
    "outputId": "c39dddb4-7ee5-40f6-debd-751d198f1807"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot with color based on final_score\n",
    "sc = plt.scatter(\n",
    "    evaluation_df['runtime_sec'],\n",
    "    evaluation_df['f1'],\n",
    "    c=evaluation_df['final_score'],\n",
    "    cmap='viridis',\n",
    "    s=80,\n",
    "    edgecolors='black'\n",
    ")\n",
    "\n",
    "# Annotate each point with the model name (index)\n",
    "for idx, row in evaluation_df.iterrows():\n",
    "    plt.text(row['runtime_sec'], row['f1'], idx, fontsize=9, ha='right', va='bottom')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('Final Score')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Run Time (seconds)')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Model Comparison: F1 vs Run Time (Colored by Final Score)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('model_comparison_plot.png', dpi=300)  # Change filename or dpi if needed\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "cwOF5BXfsiSY",
    "outputId": "4b50fd70-76b5-41c8-ab9c-ed936e2bc1ed"
   },
   "outputs": [],
   "source": [
    "evaluation_df[['f1', 'runtime_sec', 'final_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLldyS1tr_w-"
   },
   "source": [
    "confusion matrix only for the chosen Roberta model and the chosen Distilbert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "tQ9Xo6Co4G-E",
    "outputId": "1dc3db8d-84a8-4f84-c4bb-6a7cfb45e93d"
   },
   "outputs": [],
   "source": [
    "selected_rows = evaluation_df.iloc[[0, 3]]\n",
    "\n",
    "# Set up the figure for 1x2 subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot each selected confusion matrix\n",
    "for i, (idx, row) in enumerate(selected_rows.iterrows()):\n",
    "    ax = axes[i]\n",
    "    cm = row['conf_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)\n",
    "    ax.set_title(f'Confusion Matrix: {idx}')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "plt.savefig('conf_mat.png', dpi=300)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iP-eAFBs81mP"
   },
   "source": [
    "<center><h1>END</h1></center>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
